python -c 'import theano; print(theano.config)' | less

To overwrite default config, create the following file:
    $home/.theanorc

- check the layer shape
    model.layers[...].input_shape
    model.layers[...].output_shape 
model.layers is only accessible once the model is instantiated.
use .keras_shape on the output instantiated layer to get the output shape of
your tensor
    
- output of intermediate layer

# METHOD 1 create a new model with the layers that you are interested in 
from keras.models import Model
model = ...
layer_name = 'my_layer'
intermediate_layer_model = Model(inputs=model.input, 
                                 outputs=model.get_layer(layer_name).output)
intermediate_output = intermediate_layer_model.predit(data)

# METHOD 2 build keras function return the output of ceterian layer given
# certain input 
from keras import backend as K
get_3rd_layer_output = K.function([model.layers[0].input],
                                   model.layers[3].output])
layer_output = get_3rd_layer_output([x])[0]


get_3rd_layer_output = K.function([model.layers[0].input, K.learning_phase()]
                                   model.layers[3].output])
# output in test mode=0
layer_output = get_3rd_layer_output([x, 0])[0]
# output in train mode=1
layer_output = get_3rd_layer_output([x, 1])[0]


# METHOD 3
model.summary()  # print all layers and their output shape


- LTSM
stateful: means the states from samples of each batch will be reused as
initial sates for the sample in the next batch


build complex graph: 
    - model is a collection of layers
    - model can be nested inside layers
    - concat will join layers togeter


build your own network layer
    - inherit from layers.Layer 
    - implement: compute_output_shape() 
    - call() input to generate output


numpy resources:
- numpy 100 exercises: 
    https://github.com/rougier/numpy-100/blob/master/100%20Numpy%20exercises.md
- numpy quick start    
    https://docs.scipy.org/doc/numpy-dev/user/quickstart.html
- numpy tutorial
    http://www.labri.fr/perso/nrougier/teaching/numpy/numpy.html
- numpy nd array indexing
    https://docs.scipy.org/doc/numpy-1.13.0/reference/arrays.indexing.html

theano
    - numpy + symbolic math
    - decalre variable, build expr for the computation graph
    - use T.grad to compute gradient, jacobi, hessian (auto differentiation)
    - use shared, reduce, can to for recurrent net 
    - steps: 
        1. use theano.tensor (T) to define variables
        2. define expression with the variables 
        3. use theano.function to define the function to be compiled
        4. invoke the function
            6. use shared variable to modify state
            7. use scan to unroll loop compuation
            8. use normal python function as subgraph to compose the expression graph

sparse matrix
    - CSC: slice(indptr[i], indptr[i+1]) data for the ith col
        - for tall matrix
        - CSC is good for V'*M
        - not good for adding/deleting element
    - CSR: slice(indptr[i], indptr[i+1]) data for the ith row
        - for wide matrix
        - CSR is good for M*V
        - not good for adding/deleting element
    - structured_op: computation only for non-zero elements in the matrix

convlution net
    - filter bank
    - padding
    - stride
    - convolution as trapzoid matrix 
    - down sampling
    - forward/backward properation ~ W/W.T 
        ~ convolution/convolution back

GPU
    - .theanorc
        [global]
        device = cuda
        floatx = float32
    - libgpuarry for GpuArray backend
    - cuda toolchain
    - dev (1) with C CUDA api; (2) with pyCUDA
    - multiple gpus with "config.contexts" map
    
debugging tips
    - attach x.tag.test_value to debug input/output dim mismatch
    - theano.pp(), theano.printing.debugprint() to print the graph
    - pydotprint() dump the computation graph as dot file
    - use theano.printing.Print('x')(x) to print out intermediate values
    - use MonintorMode to inspect input/output of compiled function

dealing with NaN
    - lower learning weight
    - set penalty term properly
    - initialize the weights properly
    - run in NanGuardMode, DebugMode, MonitorMode

profiling
    - passing profile=True to theano.function. and call f.profile.summary()
      for a single function

graph strucutre
    - op, variable, owner
    - apply node (accessed by owner field)

loading/saving
    - all theano object are picklable
    - cPickle, compared with pickle, is faster because it is written in C.
    - use __getstate__(self): pass  __setstate__(self): pass to control which
      fields are serialized.

memory usage
    - python basic types use more memory than C datatypes
    - use "borrow=True", "return_internal_structure=True" to inplace memory
    - @profile from memory_profiler to profile memory usage per statement
    - pickle will double the memory usage
        - consider use plain txt file
        - use numpy (or pytables) arrays

Parallelization on CPU
    - blas: OMP_NUM_THREADS
    - set openmp flag to true

TO LEARN:    
    Torch/TensorFlow

